{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb98f1-96c0-43da-90cf-e1a630e373a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is an ensemble technique in machine learning?\n",
    "\n",
    "Ensemble techniques in machine learning involve combining multiple models to create a stronger, more robust model. These models can be of the same or different types, and their predictions are aggregated to improve overall performance.\n",
    "\n",
    "Q2. Why are ensemble techniques used in machine learning?\n",
    "\n",
    "Ensemble techniques are used to enhance model performance by leveraging the diversity of multiple models. They help reduce overfitting, improve generalization, and increase the stability and accuracy of predictions.\n",
    "\n",
    "Q3. What is bagging?\n",
    "\n",
    "Bagging (Bootstrap Aggregating) is an ensemble technique where multiple models are trained on different bootstrap samples (randomly sampled subsets with replacement) of the training dataset. The final prediction is often obtained by averaging or voting among the predictions of individual models.\n",
    "\n",
    "Q4. What is boosting?\n",
    "\n",
    "Boosting is an ensemble technique where multiple weak learners (models that perform slightly better than random chance) are trained sequentially. Each subsequent model focuses on correcting the errors of its predecessor, leading to a strong and accurate predictive model.\n",
    "\n",
    "Q5. What are the benefits of using ensemble techniques?\n",
    "\n",
    "Improved accuracy: Ensembles can outperform individual models.\n",
    "Robustness: Ensembles are less sensitive to noise and outliers.\n",
    "Generalization: They reduce overfitting, improving the model's ability to generalize to new data.\n",
    "Versatility: Ensembles can be applied to various types of models and tasks.\n",
    "Q6. Are ensemble techniques always better than individual models?\n",
    "\n",
    "While ensemble techniques often outperform individual models, their effectiveness depends on factors such as the diversity of base models, the quality of individual models, and the nature of the dataset. In some cases, a well-tuned individual model might perform similarly to or better than an ensemble.\n",
    "\n",
    "Q7. How is the confidence interval calculated using bootstrap?\n",
    "\n",
    "To calculate the confidence interval using bootstrap:\n",
    "\n",
    "Bootstrap Resampling: Create multiple bootstrap samples by randomly sampling with replacement from the original dataset.\n",
    "Calculate Statistic: For each bootstrap sample, calculate the statistic of interest (e.g., mean, median).\n",
    "Construct Confidence Interval: Determine the range of values that contains the desired percentage (e.g., 95%) of the bootstrap sample statistics. This range forms the confidence interval.\n",
    "Q8. How does bootstrap work, and what are the steps involved in bootstrap?\n",
    "\n",
    "Bootstrap works by resampling with replacement from the observed data to create multiple datasets, mimicking the process of drawing multiple samples from the underlying population. The steps are:\n",
    "\n",
    "Sample with Replacement: Randomly select data points from the observed dataset, allowing for duplicate entries in the new bootstrap sample.\n",
    "Calculate Statistic: Compute the desired statistic (mean, median, etc.) on the bootstrap sample.\n",
    "Repeat: Repeat steps 1 and 2 to create multiple bootstrap samples and calculate the statistic for each.\n",
    "Estimate Confidence Interval: Use the distribution of bootstrap sample statistics to estimate the confidence interval.\n",
    "\n",
    "Answer9:\n",
    "import numpy as np\n",
    "\n",
    "# Given sample data\n",
    "sample_mean = 15  # mean height of the sample\n",
    "sample_std = 2   # standard deviation of the sample\n",
    "sample_size = 50  # size of the sample\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_bootstrap_samples = 1000\n",
    "\n",
    "# Generate bootstrap samples\n",
    "bootstrap_samples = np.random.normal(loc=sample_mean, scale=sample_std, size=(num_bootstrap_samples, sample_size))\n",
    "\n",
    "# Calculate mean height for each bootstrap sample\n",
    "bootstrap_means = np.mean(bootstrap_samples, axis=1)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "# Print the results\n",
    "print(\"95% Confidence Interval for Population Mean Height:\", confidence_interval)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
